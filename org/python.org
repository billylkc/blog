#+STARTUP: content
#+OPTIONS: \n:t
#+OPTIONS: ^:{}
#+HTML_HEAD: <base target="_blank">
#+hugo_base_dir: ../
#+hugo_section: ./posts
#+hugo_weight: auto
#+hugo_auto_set_lastmod: t

#+author: Billy Lam

* Python                                                                :@python:python:

** Hosting a keyword extraction model with Flask and FastAPI     :api:model:
:PROPERTIES:
:EXPORT_FILE_NAME: model-hosting
:EXPORT_DATE: 2021-06-28
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 20
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover https://storage.googleapis.com/billylkc-blog-image/images/posts/12-model-hosting/thumbnails.jpg
:DESCRIPTION: Hosting a keyword extraction model with Flask and FastAPI. Deploy your ML model with RESTFUL API endpoints. YAKE keyword extractor. Swagger documentation. Post request with python.
:EXPORT_TITLE: Hosting a keyword extraction model with Flask and FastAPI
:SUMMARY: Hosting a keyword extraction model with Flask and FastAPI
:END:

The topic of ML deployment is rarely discussed when machine learning is taught.

<!--more-->

*** Introduction

[[https://i1.wp.com/cdn-images-1.medium.com/max/1600/1*jBjJw7jq71pw4iZkqyp2Uw.jpeg?resize=382%2C374&ssl=1]]

When you noticed learning how to build a machine learning model is not enough, you are graduating from the Data Science bootcamp.

Here is an introduction of ML model serving with Restful API endpoint.

*** Overview

In this post, we will be covering the following topics
- Yet Another Keyword Extractor
- Our ML model
- Flask API
- Testing with curl
- FastAPI
- Testing with Postman

At the end of this post, we will have an API endpoint (from ~localhost~) to *extract 20 keywords from a text paragraph* with a simple POST requests.

*** Yet Another Keyword Extractor

YAKE (Yet Another Keyword Extractor) is a *light-weight unsupervised automatic keyword extraction method*. It rests on *text statistical features* extracted from single document to select the most important keywords.

It is quite useful to extract details from a text paragraph or use it as an alternatives to labeled data (When you don't have any).

**** Main Features
- Unsupervised approach
- Single-Document
- Corpus-Independent
- Domain and Language Independent

**** Heuristic Approach

As opposed to other keyword extraction algorithms like tf-idf, one of the strong selling points of YAKE is its ability to extract keywords within a *single document*. It can be easily applied to single paragraph or document, without the existence of a corpus, dictionary or other external collections.

Here are some components of the method outlined in the paper.

1. Text preprocessing
2. Feature extraction
3. **Individual terms score**
4. Candidate keywords list generation
5. Data deduplication
6. Ranking

Some interesting ideas during the term scoring process.
- *Casing*

  Reflects the casing aspect of a word.

- *Word Positions*

  Values more on the words occurring at the beginning of the documents, on the assumption that relevant keywords often concentrate more at the beginning.

- *Word Relatedness to Context*

  Computes the number of different terms that occur to the *left and right side* of the candidate word. The more the **number of different terms co-occur with the candidate word** (on both sides), the more the meaningness it is likely to be. Similar idea applies to different sentences as well.

For more details, you can check out the paper in the reference page.

Reference: [[https://github.com/LIAAD/yake][Github]] and [[https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf][Paper]]


*** Our ML model
As the main purpose of this post is to demonstrate how to *deploy a model with API endpoints*, I will use a simple wrapper of ~yake.KeywordExtrator~ function to act as our machine learning model.

- Installation

#+BEGIN_SRC python
pip install yake
#+END_SRC

- Our model

#+BEGIN_SRC python
    import yake
    def ExtractKeywords(text):
        """ Extract 20 keywords from the input text """

        language = "en"
        max_ngram_size = 2
        deduplication_thresold = 0.3
        deduplication_algo = "seqm"
        windowSize = 1
        numOfKeywords = 20

        custom_kw_extractor = yake.KeywordExtractor(
                lan=language,
                n=max_ngram_size,
                dedupLim=deduplication_thresold,
                dedupFunc=deduplication_algo,
                windowsSize=windowSize,
                top=numOfKeywords,
                features=None,
        )
        kws = custom_kw_extractor.extract_keywords(text)
        keywords = [x[0] for x in kws]  # kws is in tuple format, extract the text part

        return keywords
#+END_SRC

*** Flask API

Having a ML model ready is only half the job done. A model is useful only when someone is able to use it.

Now we are going to serve our model with a *Restful API endpoint* using *Flask*. The package uses a simple decorator format for you to define an endpoint, e.g. ~@app.route('/keywords', methods = ['POST', 'GET'])~.

Here we specify our endpoint to accept both ~GET~ and ~POST~ requests. The GET request will print a curl statement, and the POST request will extract the keywords.

- installation
#+BEGIN_SRC python
pip install flask
#+END_SRC

- serve with ~/keywords~ endpoint
#+BEGIN_SRC python
  from flask import Flask, request
  import yake

  app = Flask(__name__)

  def ExtractKeywords(text):
      """ Extract 20 keywords from the input text """

      language = "en"
      max_ngram_size = 2
      deduplication_thresold = 0.3
      deduplication_algo = "seqm"
      windowSize = 1
      numOfKeywords = 20

      custom_kw_extractor = yake.KeywordExtractor(
              lan=language,
              n=max_ngram_size,
              dedupLim=deduplication_thresold,
              dedupFunc=deduplication_algo,
              windowsSize=windowSize,
              top=numOfKeywords,
              features=None,
      )
      kws = custom_kw_extractor.extract_keywords(text)
      keywords = [x[0] for x in kws]  # kws is in tuple format, extract the text part

      return keywords

  @app.route('/keywords', methods = ['POST', 'GET'])
  def keywords():
      if request.method == "POST":
              json_data = request.json
              text = json_data["text"]
              kws = ExtractKeywords(text)

              # return a dictionary
              response = {"keyowrds": kws}
              return response

      elif request.method == "GET":
              response = """
              Extract 20 keywords from a long text. Try with curl command. <br/><br/><br/>

              curl -X POST http://127.0.0.1:5001/keywords -H 'Content-Type: application/json' \
              -d '{"text": "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled 0 and 1. In the logistic model, the log-odds (the logarithm of the odds) for the value labeled 1 is a linear combination of one or more independent variables (predictors); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled 1 can vary between 0 (certainly the value 0) and 1 (certainly the value 1), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio."}'
              """
              return response

      else:
              return "Not supported"

  if __name__ == '__main__':
      app.run(host="0.0.0.0", port=5001, debug=True)

#+END_SRC

- Host the server with port 5001 ~app.run(host="0.0.0.0", port=5001, debug=True)~
#+BEGIN_SRC bash
python main.py
#+END_SRC

Reference - [[https://flask.palletsprojects.com/en/2.0.x/][Flask]]

*** Testing with curl

Let's use a paragraph from wikipedia of the ~Logistic Regression~ page as an input of our curl command and pass it as an argument ~text~ (Double quote removed) to the model.

#+BEGIN_SRC bash
curl -X POST http://127.0.0.1:5001/keywords -H 'Content-Type: application/json' \
  -d '{"text": "Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression[1] (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled 0 and 1. In the logistic model, the log-odds (the logarithm of the odds) for the value labeled 1 is a linear combination of one or more independent variables (predictors); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled 1 can vary between 0 (certainly the value 0) and 1 (certainly the value 1), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio."}'
#+END_SRC

**** Demo



[[https://storage.googleapis.com/billylkc-blog-image/images/posts/12-model-hosting/flask-q.gif]]



*Results*
#+BEGIN_SRC json
{
  "keywords": [
    "logistic model",
    "variable",
    "regression",
    "binary dependent",
    "labeled",
    "form",
    "odds",
    "exist",
    "basic",
    "complex",
    "indicator",
    "probability",
    "log-odds scale",
    "sigmoid function",
    "converts log-odds",
    "Mathematically",
    "scales",
    "alternative",
    "defining",
    "constant"
  ]
}

#+END_SRC



The result is actually quite good given its unsupervised nature. We can see some important keywords like *log-odds, sigmoid function and binary* in the result.


*** FastAPI

Apart from Flask that we just introduced, there is another popular package to host API endpoints - FastAPI. \\

*FastAPI* is a modern, fast and popular web framework for building APIs based on standard Python type hints. It is a high performant package, and it is on par with some popular framework written in **NodeJS** and **Go**.

Let's try to host our keywords model again with FastAPI.

- Key steps

  + Both *Input* and *Output Object* inherit ~pydantic.Basemodel~ object
  + Use python *type hints* ~str~ (input) and ~List[str]~ (output) to define field types of the objects
  + Use Objects as input/output parameter =Response/Paragraph=

#+BEGIN_SRC python
  # Input object with a text field
  class Paragraph(BaseModel):
      text: str

  # Output object with keywords as field
  class Response(BaseModel):
      keywords: List[str]

  @app.post("/keywords", response_model=Response)
  def keywords_two(p: Paragraph):
      ...
      return Response(keywords=kw)

#+END_SRC

- Code
#+BEGIN_SRC python
  from fastapi import FastAPI
  from pydantic import BaseModel
  from typing import List
  import yake

  # Input
  class Paragraph(BaseModel):
      text: str

  # Output
  class Response(BaseModel):
      keywords: List[str]

  app = FastAPI()

  def ExtractKeywords(text):
      """ Extract 20 keywords from the input text """

      language = "en"
      max_ngram_size = 2
      deduplication_thresold = 0.3
      deduplication_algo = "seqm"
      windowSize = 1
      numOfKeywords = 20

      custom_kw_extractor = yake.KeywordExtractor(
          lan=language,
          n=max_ngram_size,
          dedupLim=deduplication_thresold,
          dedupFunc=deduplication_algo,
          windowsSize=windowSize,
          top=numOfKeywords,
          features=None,
      )
      kws = custom_kw_extractor.extract_keywords(text)
      keywords = [x[0] for x in kws]  # kws is in tuple format, extract the text part

      return keywords


  @app.post("/keywords", response_model=Response)
  def keywords(p: Paragraph):
      kw = ExtractKeywords(p.text)
      return Response(keywords=kw)

#+END_SRC

- Host

  a) Install fastapi and uvicorn
  #+BEGIN_SRC bash
    pip install fastapi
    pip install uvicorn
  #+END_SRC

  b) Host FastAPI with uvicorn
  #+BEGIN_SRC bash
    uvicorn main:app --host 0.0.0.0 --port 5001 --reload --debug --workers 3
  #+END_SRC

- Documentation

  FastAPI creates a documentation page for you by default using the [[https://swagger.io/tools/swagger-ui/][Swagger UI]]. You can open the documentation page with ~http://localhost:5001/docs~.
  If you follow the schema definition, you can have a nice looking API documentation with some examples as well.

#+CAPTION: Auto generated Swagger API doc
[[https://storage.googleapis.com/billylkc-blog-image/images/posts/12-model-hosting/swagger.png]]

Reference - [[https://fastapi.tiangolo.com/https://fastapi.tiangolo.com/][FastAPI]] and [[https://fastapi.tiangolo.com/tutorial/schema-extra-example/][Declare Request Example]]

*** Testing with Postman

**** Demo


[[https://storage.googleapis.com/billylkc-blog-image/images/posts/12-model-hosting/fastapi-q.gif]]

*** Complete example

You can find the complete examples here - [[https://github.com/billylkc/blogposts/blob/7_flask_api/main.py][Flask]] and [[https://github.com/billylkc/blogposts/blob/8_fastapi/main.py][FastAPI]]


*** Final thoughts

Here we introduced two different frameworks (*Flask* and *FastAPI*) to serve our keyword extraction model on our local machine. While Flask being more popular among web developers, and FastAPI being more performant, it is both pretty easy to use.

Hopefully you can see how easy it is for you to host the model using the frameworks. If you have any questions or feedback, feel free to leave a comment.

Happy Coding!


_

Reference:
- Photo by [Ilyuza Mingazova](https://unsplash.com/@ilyuza?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/serfing?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- YAKE - [[https://github.com/LIAAD/yake][Github]] and [[https://repositorio.inesctec.pt/bitstream/123456789/7623/1/P-00N-NF5.pdf][Paper]]
- Flask and FastAPI - [[https://flask.palletsprojects.com/en/2.0.x/][Here]] and [[https://fastapi.tiangolo.com/https://fastapi.tiangolo.com/][Here]]



** TODO Definitive guide to python debugging with pdb and VS Code   :workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: python-debugger
:EXPORT_DATE: 2021-06-21
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 15
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover
:DESCRIPTION: Definitive guide to python debugging with pdb and VS Code for Data Scientist.
:EXPORT_TITLE: Definitive guide to python debugging with pdb and VS Code for Data Scientist.
:SUMMARY: Definitive guide to python debugging with pdb and VS Code for Data Scientist.

:END:

I don't know where you are, I don't know how you work. But I will find you, And.. \\


I will fix you - TheRaxTon

<!--more-->

*** Introduction

The only way to be a zero bug programmer is to **not write any code**. No Cap.

Tooling plays an important roles in debugging.

I’ve seen many practitioners try to spot bugs using only print statements instead of actual debugging tools. Sometimes it is a personal preference, but sometimes you just do not know it exists.

This post is a gentle introduction of the debugging tools ~pdb~ and ~debugging mode~ in Visual Studio Code. Hopefully you will have a better understanding on what functionalities a good debugger offers.

*** Functionalities
*** pdb

The Python debugger ~pdb~ implements an **interactive debugging environment** that you can use with any of your programs written in Python.

With features that let you pause your program, look at what values your variables are set to, and go through program execution in a discrete step-by-step manner, you can more fully understand what your program is doing and find bugs that exist in the logic or troubleshoot known issues.

Here is a little Cheatsheet.

Basic
#+CAPTION: Basic commands
#+ATTR_HTML: :class table table-striped table-dark
| Command               | Description                 |
|-----------------------+-----------------------------|
| **(h)** help          | List all available commands |
| **(q)** quit          | Quit debug session          |
| **(l)** list          | List a few lines            |
| **(b 10)** breakpoint | Set break point at line 10  |


#+CAPTION: Navigation and debugging
| Command                          | Description                   |
|----------------------------------+-------------------------------|
| **(l)** list                     | List a few lines              |
| **(n)** next                     | Move to next line             |
|----------------------------------+-------------------------------|
| **(s)** step                     | Step into function            |
| **(j)** jump                     | Jump                          |
| **ENTER**                        | Repeat last command           |
| **(p)** print **/expr/**         | Print variable /expr/         |
| **(pp)** pretty print **/expr/** | Pretty Print /expr/           |
|----------------------------------+-------------------------------|
| **(b)** break                    | Show all break point          |
| **(b)** break **/num/**          | Set break point at line /num/ |
| **(c)** continue                 | Continue til next break point |
|----------------------------------+-------------------------------|
| **locals()**                     | List local variables          |

Reference: [[https://appletree.or.kr/quick_reference_cards/Python/Python%20Debugger%20Cheatsheet.pdf][Python Debugger Cheatsheet]]

*** Demo

*** Debugging in visual studio code

*** Final thoughts

By now you should have a basic understanding of how debugging works in Python, as well as the tools available in pdb in standard library, to graphical debugging in IDE like visual studio code. Using a debugger for your own codebases is really going to supercharge your productivity.


Happy Coding!


_

Reference:
-


** Getting HKEX data with Quandl in Python                             :api:
:PROPERTIES:
:EXPORT_FILE_NAME: hkex-with-python
:EXPORT_DATE: 2021-06-21
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 30
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover https://storage.googleapis.com/billylkc-blog-image/images/posts/11-quandl/thumbnails.jpg
:DESCRIPTION: Getting HKEX data with Quandl in Python. Historical daily HKEX data using API. Stock exchange in Yahoo Finance Hong Kong.
:EXPORT_TITLE: Getting HKEX data with Quandl in Python
:SUMMARY: Getting HKEX data with Quandl in Python
:END:

Free alternatives to get HKEX daily stock data after Yahoo Finance

<!--more-->

*** Introduction
Free Quandl Stock API for HK stock data.

Getting a stable source of daily stock data is all I needed. And since Yahoo Finance shutdown their API, I have been looking for a free alternative data source. In this post, I will quickly show you how we can get some daily stock price data from HK Stock Exchange Market with the [[https://www.quandl.com/][Quandl]] Python API.


*** Overview

In this article, we will be covering the following topics

- Introduction to Quandl Python API

- Basic setup and quick exploration

- Getting 10 days of records for all stocks

*** Quandl Python API
**Quandl** is a marketplace for financial, economic and alternative data delivered in modern formats for today's financial analysts. It provides free API (Python, Excel, Matlab, R, etc..) for registered users. You can register and get a **free API key** - [[https://www.quandl.com/sign-up][Here]]. The setup should be quite straight forward.

Here is an usage table of the free tier for your reference. We will be using the free tier as an example.

#+CAPTION: Rate Limits
|---------+-------------------+--------------------|
| Tier    | Requests per Day  | Concurrent Request |
|---------+-------------------+--------------------|
| Free    | 50,000 calls/day  | 1                  |
| Premium | 720,000 calls/day | -                  |
|---------+-------------------+--------------------|

Reference: [[https://help.quandl.com/article/132-how-much-does-quandl-data-cost][Quandl Pricing]] and [[https://docs.quandl.com/docs][Usage rate]]

*** Basic setup and quick exploration

**** a) Install package

#+BEGIN_EXAMPLE
pip install quandl
#+END_EXAMPLE

**** b) Set up environment variable

Given that the free API key is not a 'secret' secret. Putting the token in an environment variable would serve the purpose for this demo.


- Open your =~/.bashrc= and add the following line **(Persistent)** \\

In =~/.bashrc=


  #+BEGIN_SRC bash
    export QUANDL_TOKEN="YOUR_API_TOKEN"
  #+END_SRC

In command line


  #+BEGIN_SRC bash
    source ~/.bashrc
  #+END_SRC

- Or simply run in the command line **(One off)**


  #+BEGIN_SRC bash
    export QUANDL_TOKEN="YOUR_API_TOKEN"
  #+END_SRC

**** c) Quick demo

It is quite straight forward to get the data, just call it with the **stock code** ~HKEX/00005~ and ~quandl.get~ function.

#+BEGIN_SRC python :session :results value
  import os
  import requests
  import pandas as pd
  import quandl
  import numpy as np

  pd.set_option('display.max_columns', None)
  quandl.ApiConfig.api_key = os.environ['QUANDL_TOKEN']

  num = 5     # HSBC
  code = str(num).zfill(5)
  code_str = "HKEX/{}".format(code)
  data = quandl.get(code_str, rows = 10)
  data['Code'] = code

  print(data)

#+END_SRC



*** Getting data

Let's extend our example and try to get **all the stocks** in the past 10 days.

**** a) Get a list of all codes

As the list of all listed companies are likely to change, let's get the latest one from the HKEX page - [[https://www.hkexnews.hk/sdw/search/stocklist_c.aspx?sortby=stockcode&shareholdingdate=20210621][Here]].

We are going to the page, get the text from the cells of the table, then use regular expression to capture the stock code with 5 digits value (e.g. 00005). I will leave the introduction to BeautifulSoup for another post 😁.

#+BEGIN_SRC python
  from bs4 import BeautifulSoup
  from datetime import datetime
  from typing import List
  import requests
  import re
  import pandas as pd
  import numpy as np
  import quandl


  def get_codes() -> List[int]:

      """
      Get all the codes from the listed companies in HK main board from HKEX page

      Args:
              None

      Returns:
              codes ([]int): List of codes in HKEX main board

      Example:
              codes = get_codes()

      Data preview:
              [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, ..]
      """

      regex = re.compile(r"\s*(\d{5})(.*)")  # Get 5 digit codes only
      url = "https://www.hkexnews.hk/sdw/search/stocklist_c.aspx?sortby=stockcode&shareholdingdate={}".format(
              datetime.today().strftime("%Y%m%d")
      ) # derive url, e.g. https://www.hkexnews.hk/sdw/search/stocklist_c.aspx?sortby=stockcode&shareholdingdate=20210621

      res = requests.get(url)
      soup = BeautifulSoup(res.text, "html.parser")

      codes = []
      for s in soup.select("table.table > tbody > tr"):
              text = s.get_text().replace(" ", "").strip()  # Replace extra spaces
              matchResult = regex.search(text)

              if matchResult:
                      code = int(matchResult.group(1).lstrip("0"))  # Convert to int, e.g. 00005 to 5

                      if code <= 10000:  # main board only
                              codes.append(code)

      return codes

#+END_SRC

**** b) Get individual stock (First try)

Here we are having a function with input as stock code (int) and return a dataframe of the historical records.

**Remarks**: This implementation is **rate limited** with the package function =quandl.get= as free account.

#+BEGIN_SRC python
  import pandas as pd
  import numpy as np
  import quandl

  pd.set_option("display.max_columns", None)
  quandl.ApiConfig.api_key = os.environ["QUANDL_TOKEN"]

  def get_stock(num: int, nrow: int = 10) -> pd.DataFrame:

      """
      Call Quandl API to get the historical data for the stock number (Rate limited)
      """

      code = str(num).zfill(5)
      code_str = "HKEX/{}".format(code)  # e.g. HKEX/00005

      try:
              data = quandl.get(code_str, rows = nrow)
              data['Code'] = code

              col_name = data.columns.tolist()
              clean_col_name = [re.sub(r'\W+', '', x) for x in col_name]  # Replace special character in column name
              col_dict = dict(zip(col_name, clean_col_name))

              data.rename(columns=col_dict, inplace=True)
              print("Finished getting code - {}".format(code))

              return(data)

      except Exception as e:
              print("No records - {}".format(code))
              print(e)

#+END_SRC

**** c) Get individual stock (Second Try)

Turns out there is no rate limits for the GET requests. Let's do it again with =requests.get= and =io.StringIO=.

#+BEGIN_SRC python
    from datetime import datetime
    import pandas as pd
    import numpy as np
    import requests
    import io
    import os

    quandl.ApiConfig.api_key = os.environ["QUANDL_TOKEN"]

    def get_stock(num: int, nrow: int = 10) -> pd.DataFrame:

        """
        Call Quandl API to get the historical data for the stock number using GET requests

        Args:
                num (int): Stock num, e.g. 5
                nrow (int): No of rows specified in the API calls. Default 10

        Returns:
                data (Dataframe): Dataframe returned from Quandl API

        Example:
                data = get_stock(num=1, nrow=10)

        TODO:
                Add date parameter to specify the latest date of the call

        Data preview:
                                      NominalPrice NetChange Change    Bid    Ask   PEx   High    Low  PreviousClose  ShareVolume000  Turnover000 LotSize   code
          Date
          2019-03-19         80.45      None   None  80.40  80.45  None  81.15  80.20          80.95          7374.0     593781.0    None  00001
          2019-03-20         82.50      None   None  82.50  82.55  None  83.30  80.30          80.45         12420.0    1018144.0    None  00001
          2019-03-21         81.60      None   None  81.60  81.75  None  83.50  81.60          82.50         12224.0    1009254.0    None  00001
          2019-03-22         83.80      None   None  83.75  83.80  None  84.65  82.85          81.60         13478.0    1124179.0    None  00001
        """
        today = datetime.today().strftime("%Y-%m-%d")  # e.g. 2021-06-23
        code = str(num).zfill(5)
        code_str = "HKEX/{}".format(code)  # e.g. HKEX/00005

        # Get from csv
        endpoint = "https://www.quandl.com/api/v3/datasets/{}/data.csv?limit={}&end_date={}&order={}&api_key={}".format(
                code_str,
                nrow,
                today,
                "desc",
                quandl.ApiConfig.api_key,
        )
        r = requests.get(endpoint).content
        data = pd.read_csv(io.StringIO(r.decode("utf-8")))

        data["Code"] = code

        # Check if there is any error message
        col_name = data.columns.tolist()
        if "message" in col_name:
                raise Exception("Incorrect stock code - {}".format(code))

        clean_col_name = [re.sub(r"\W+", "", x) for x in col_name]  # Replace special character in column name
        col_dict = dict(zip(col_name, clean_col_name))

        data.rename(columns=col_dict, inplace=True)
        print("Finished getting code - {}".format(code))

        return data

#+END_SRC


**** d) Get all stocks

We finally loop through all the codes and concat the results to a single dataframe.

#+BEGIN_SRC python
  def get_all_stock(nrow: int = 10) -> pd.DataFrame:
      """ Loop through the list of codes, and concat the results to a single dataframe. """
      codes = get_codes()
      codes = codes[0:10] # Hardcorded 20 stocks for demostration.

      # Initialize result dataframe
      result = pd.DataFrame()
      for code in codes:
              try:
                      data = get_stock(code, nrow)
                      result = pd.concat([result, data], sort=True)

              except Exception as e:
                      print("No records")
                      print(e)

      return result

#+END_SRC

**** e) Complete example

#+BEGIN_SRC python
    from bs4 import BeautifulSoup
    from datetime import datetime
    from typing import List
    import requests
    import re
    import os
    import io
    import pandas as pd
    import numpy as np
    import quandl

    pd.set_option("display.max_columns", None)
    quandl.ApiConfig.api_key = os.environ["QUANDL_TOKEN"]


    def get_codes() -> List[int]:

        """
        Get all the codes from the listed companies in HK main board from HKEX page

        Args:
                None

        Returns:
                codes ([]int): List of codes in HKEX main board

        Example:
                codes = get_codes()

        Data preview:
                [1, 2, 3, 4, 5, 6, 11,..]
        """

        regex = re.compile(r"\s*(\d{5})(.*)")  # Get 5 digit codes only
        url = "https://www.hkexnews.hk/sdw/search/stocklist_c.aspx?sortby=stockcode&shareholdingdate={}".format(
                datetime.today().strftime("%Y%m%d")
        )  # derive url, e.g. https://www.hkexnews.hk/sdw/search/stocklist_c.aspx?sortby=stockcode&shareholdingdate=20210621

        res = requests.get(url)
        soup = BeautifulSoup(res.text, "html.parser")

        codes = []
        for s in soup.select("table.table > tbody > tr"):
                text = s.get_text().replace(" ", "").strip()  # Replace extra spaces
                matchResult = regex.search(text)

                if matchResult:
                        code = int(matchResult.group(1).lstrip("0"))  # Convert to int, e.g. 00005 to 5

                        if code <= 10000:  # main board only
                                    codes.append(code)

        return codes


    def get_stock(num: int, nrow: int = 10) -> pd.DataFrame:

        """
        Call Quandl API to get the historical data for the stock number using GET requests

        Args:
           num (int): Stock num, e.g. 5
           nrow (int): No of rows specified in the API calls. Default 10

        Returns:
           data (Dataframe): Dataframe returned from Quandl API

        Example:
           data = get_stock(num=1, nrow=10)

        TODO:
           Add date parameter to specify the latest date of the call

        Data preview:
                                      NominalPrice NetChange Change    Bid    Ask   PEx   High    Low  PreviousClose  ShareVolume000  Turnover000 LotSize   code
          Date
          2019-03-19         80.45      None   None  80.40  80.45  None  81.15  80.20          80.95          7374.0     593781.0    None  00001
          2019-03-20         82.50      None   None  82.50  82.55  None  83.30  80.30          80.45         12420.0    1018144.0    None  00001
          2019-03-21         81.60      None   None  81.60  81.75  None  83.50  81.60          82.50         12224.0    1009254.0    None  00001
          2019-03-22         83.80      None   None  83.75  83.80  None  84.65  82.85          81.60         13478.0    1124179.0    None  00001
        """

        today = datetime.today().strftime("%Y-%m-%d")  # e.g. 2021-06-23
        code = str(num).zfill(5)
        code_str = "HKEX/{}".format(code)  # e.g. HKEX/00005

        # Get from csv
        endpoint = "https://www.quandl.com/api/v3/datasets/{}/data.csv?limit={}&end_date={}&order={}&api_key={}".format(
                code_str,
                nrow,
                today,
                "desc",
                quandl.ApiConfig.api_key,
        )
        r = requests.get(endpoint).content
        data = pd.read_csv(io.StringIO(r.decode("utf-8")))

        data["Code"] = code

        # Check if there is any error message
        col_name = data.columns.tolist()
        if "message" in col_name:
                raise Exception("Incorrect stock code - {}".format(code))

        clean_col_name = [re.sub(r"\W+", "", x) for x in col_name]  # Replace special character in column name
        col_dict = dict(zip(col_name, clean_col_name))

        data.rename(columns=col_dict, inplace=True)
        print("Finished getting code - {}".format(code))

        return data


    def get_all_stock(nrow: int = 10) -> pd.DataFrame:

        """ Loop through the list of codes, and concat the results to a single dataframe. """

        codes = get_codes()
        codes = codes[0:20]  # Hardcorded 20 stocks for demostration.

        # Initialize result dataframe
        result = pd.DataFrame()

        for code in codes:
                try:

                        data = get_stock(code, nrow)
                        result = pd.concat([result, data], sort=True)
                        print("=========================")
                        print(code)
                        print(data.head())

                except Exception as e:
                        print("No records")
                        print(e)

        return result


    def main():
        df = get_all_stock()
        print(df)


    if __name__ == "__main__":
        main()

#+END_SRC


The complete code example can be found - [[https://github.com/billylkc/blogposts/blob/6_quandl_py/main.py][Here]]


*Demo* \\


[[https://storage.googleapis.com/billylkc-blog-image/images/posts/11-quandl/demo-q.gif]]

*** Final Thoughts

With the example here, you should be able to get a daily update of HKEX stock data for analysis. It would be quite easy to save the data into a DBMS like mysql or postgresql too.

Happy Coding!


_


Reference
- Photo by [Jamie Street](https://unsplash.com/@jamie452?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/stock?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)



** Python Cheatsheet                                           :python:workflow:
:PROPERTIES:
:EXPORT_FILE_NAME: python-cheatsheet
:EXPORT_DATE: 2021-06-17
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 30
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover https://storage.googleapis.com/billylkc-blog-image/images/posts/9-cheatsheet/thumbnails.jpg
:DESCRIPTION: Python Cheatsheet
:EXPORT_TITLE: Python Cheatsheet
:SUMMARY: Python Cheatsheet
:END:

Dont ask me about the syntax. I look it up on Google and API documentations. And now ~cht.sh~ too.

<!--more-->

*** Introduction

If you are having trouble to remember the exact syntax no matter how many times you've used it, you are not alone.

There is a community driven programming cheatsheet, so you can lookup the common usage of the function and it gives you a quick example to refresh your memories.

Introducing the ultimate programming cheatsheet - *cheat.sh* ([[https://cht.sh/][Official Site]]).
#+BEGIN_EXAMPLE
      _                _         _    __
  ___| |__   ___  __ _| |_   ___| |__ \ \      The only cheat sheet you need
 / __| '_ \ / _ \/ _` | __| / __| '_ \ \ \     Unified access to the best
| (__| | | |  __/ (_| | |_ _\__ \ | | |/ /     community driven documentation
 \___|_| |_|\___|\__,_|\__(_)___/_| |_/_/      repositories of the world

#+END_EXAMPLE

*** Why Cheatsheet
- **Straight to the point.** Quickly give you some useful code snippets.

- **Efficiency.** Stay in your editor while searching.

- **Easy context switching.** Extremely useful when you need to constantly switching between different programming languages. (e.g. Python, R, Spark, etc..)

*** Some Examples

1) Python group by lambda

   ~curl http://cht.sh/python/group+by+lambda~

   #+BEGIN_SRC python

     #  The apply method itself passes each "group" of the groupby object as
     #  the first argument to the function. So it knows to associate 'Weight'
     #  and "Quantity" to `a` and `b` based on position. (eg they are the 2nd
     #  and 3rd arguments if you count the first "group" argument.

     df = pd.DataFrame(np.random.randint(0,11,(10,3)), columns = ['num1','num2','num3'])
     df['category'] = ['a','a','a','b','b','b','b','c','c','c']
     df = df[['category','num1','num2','num3']]
     df

     category  num1  num2  num3
     0        a     2     5     2
     1        a     5     5     2
     2        a     7     3     4
     3        b    10     9     1
     4        b     4     7     6
     5        b     0     5     2
     6        b     7     7     5
     7        c     2     2     1
     8        c     4     3     2
     9        c     1     4     6

     gb = df.groupby('category')

     #  implicit argument is each "group" or in this case each category

     gb.apply(lambda grp: grp.sum())

     #  The "grp" is the first argument to the lambda function
     #  notice I don't have to specify anything for it as it is already,
     #  automatically taken to be each group of the groupby object

     category  num1  num2  num3
     category
     a             aaa    14    13     8
     b            bbbb    21    28    14
     c             ccc     7     9     9

     #  So apply goes through each of these and performs a sum operation

     print(gb.groups)
     {'a': Int64Index([0, 1, 2], dtype='int64'), 'b': Int64Index([3, 4, 5, 6], dtype='int64'), 'c': Int64Index([7, 8, 9], dtype='int64')}

     print('1st GROUP:\n', df.loc[gb.groups['a']])
     1st GROUP:
     category  num1  num2  num3
     0        a     2     5     2
     1        a     5     5     2
     2        a     7     3     4

     print('SUM of 1st group:\n', df.loc[gb.groups['a']].sum())

     SUM of 1st group:
     category    aaa
     num1         14
     num2         13
     num3          8
     dtype: object

     #  Notice how this is the same as the first row of our previous operation
     #
     #  So apply is _implicitly_ passing each group to the function argument
     #  as the first argument.
     #
     #  From the [docs](https://pandas.pydata.org/pandas-
     #  docs/stable/generated/pandas.core.groupby.GroupBy.apply.html)
     #
     #  > GroupBy.apply(func, *args, **kwargs)
     #  >
     #  > args, kwargs : tuple and dict
     #  >> Optional positional and keyword arguments to pass to func
     #
     #  Additional Args passed in "\*args" get passed _after_ the implicit
     #  group argument.
     #
     #  so using your code

     gb.apply(lambda df,a,b: sum(df[a] * df[b]), 'num1', 'num2')

     category
     a     56
     b    167
     c     20
     dtype: int64

     #  here 'num1' and 'num2' are being passed as _additional_ arguments to
     #  each call of the lambda function
     #
     #  So apply goes through each of these and performs your lambda operation

     # copy and paste your lambda function
     fun = lambda df,a,b: sum(df[a] * df[b])

     print(gb.groups)
     {'a': Int64Index([0, 1, 2], dtype='int64'), 'b': Int64Index([3, 4, 5, 6], dtype='int64'), 'c': Int64Index([7, 8, 9], dtype='int64')}

     print('1st GROUP:\n', df.loc[gb.groups['a']])

     1st GROUP:
     category  num1  num2  num3
     0        a     2     5     2
     1        a     5     5     2
     2        a     7     3     4

     print('Output of 1st group for function "fun":\n',
           fun(df.loc[gb.groups['a']], 'num1','num2'))

     Output of 1st group for function "fun":
     56

     #  [RSHAP] [so/q/47551251] [cc by-sa 3.0]

   #+END_SRC

2) R ggplot scatter

   ~curl http://cht.sh/r/ggplot2+scatter~

   #+BEGIN_SRC r

     # question_id: 7714677
     # One way to deal with this is with alpha blending, which makes each
     # point slightly transparent. So regions appear darker that have more
     # point plotted on them.
     #
     # This is easy to do in `ggplot2`:

     df <- data.frame(x = rnorm(5000),y=rnorm(5000))
     ggplot(df,aes(x=x,y=y)) + geom_point(alpha = 0.3)

     # ![enter image description here][1]
     #
     # Another convenient way to deal with this is (and probably more
     # appropriate for the number of points you have) is hexagonal binning:

     ggplot(df,aes(x=x,y=y)) + stat_binhex()

     # ![enter image description here][2]
     #
     # And there is also regular old rectangular binning (image omitted),
     # which is more like your traditional heatmap:

     ggplot(df,aes(x=x,y=y)) + geom_bin2d()

     # [1]: http://i.stack.imgur.com/PJbMn.png
     # [2]: http://i.stack.imgur.com/XyWw1.png
     #
     # [joran] [so/q/7714677] [cc by-sa 3.0]

   #+END_SRC

   #+attr_html: :width 250px
   [[http://i.stack.imgur.com/PJbMn.png]]

   #+attr_html: :width 250px
   [[http://i.stack.imgur.com/XyWw1.png]]

3) PySpark dataframe filter

   ~curl http://cht.sh/pyspark/filter~

   #+BEGIN_SRC python

     /*
      * Pyspark: Filter dataframe based on multiple conditions
      *
      * <!-- language-all: lang-python -->
      *
      * Your logic condition is wrong. IIUC, what you want is:
      */

     import pyspark.sql.functions as f

     df.filter((f.col('d')<5))\
         .filter(
             ((f.col('col1') != f.col('col3')) |
              (f.col('col2') != f.col('col4')) & (f.col('col1') == f.col('col3')))
         )\
         .show()

     /*
      * I broke the filter() step into 2 calls for readability, but you could
      * equivalently do it in one line.
      *
      * Output:
      */

     +----+----+----+----+---+
     |col1|col2|col3|col4|  d|
     +----+----+----+----+---+
     |   A|  xx|   D|  vv|  4|
     |   A|   x|   A|  xx|  3|
     |   E| xxx|   B|  vv|  3|
     |   F|xxxx|   F| vvv|  4|
     |   G| xxx|   G|  xx|  4|
     +----+----+----+----+---+

     /* [pault] [so/q/49301373] [cc by-sa 3.0] */

   #+END_SRC

*** My Workflow

- Have my emacs setup with left pane as **code** and right pane as **command line console**

- Set up **alias** to run go and python program with less keystrokes
  - alias ~pp~ as ~python main.py~
  - alias ~gg~ as ~go run main.go~

- Created an **utility** command line program and alias to quickly call cheatsheet with ~chp sth~ (~curl http://cht.sh/python/sth~) and ~chg sth~ (~curl http://cht.sh/go/sth~)

*** Demo

Quick demo to create a dummy python dataframe
[[https://storage.googleapis.com/billylkc-blog-image/images/posts/9-cheatsheet/cheatsheet_quick.gif]]

*** Final Thoughts

Hopefully you find it useful too. \\
Happy Coding!


_

Reference
- Reference Photo by [cottonbro](https://www.pexels.com/@cottonbro?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) on [Pexels](https://www.pexels.com/photo/white-printer-paper-on-brown-round-table-7128752/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)


** Docstrings                                                       :python:
:PROPERTIES:
:EXPORT_FILE_NAME: docstrings
:EXPORT_DATE: 2021-06-19
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 40
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover https://storage.googleapis.com/billylkc-blog-image/images/posts/10-docstrings/thumbnails.jpg
:DESCRIPTION: What is docstrings?
:EXPORT_TITLE: What is docstrings?
:SUMMARY:  What is docstrings?
:END:

Code is more often read than written.

<!--more-->

*** Introduction

Learn to write a good function documentation.

Even if you don’t intend anybody else to read your code, there’s still a very good chance that somebody will have to stare at your code and figure out what it does. That person is probably going to be you too, twelve months from now. So be kind to yourself, and start writing some good code and documentation.

Docstrings are the string literals that appear right after the definition of a function, method, class, or module. To me, it is like **a summary of what a function does**. Sometimes I just dont care what the actual implementation is (At first at least 🙂), just tell me what the input, output, and a short descriptin of what it does, before even asking me to have a look at your code.

Let's have a look at the examples in different languages.

*** Python Docstrings
**** a) One-liner docstring

Use short one-liner description for functions that you think is intuitive.
But always always write a docstings for your functions.

#+BEGIN_SRC python
  def square(n):
      """ Takes an integer and return a square of it """
      return n**2
#+END_SRC

**** b) Detailed docstring

Make sure to make it clear that what is the **input** and **output** of your function, and more importantly is to include the **type** as well.
I usually include the followings.

- Description
- Arguments with types
- Return value
- (Optional) Detailed steps
- (Optional) Example and output


#+BEGIN_SRC python
  from datetime import datetime
  from dateutil.rrule import rrule, MONTHLY
  from typing import List

  def get_months_between_dates(start: str, end:str) -> List:

      """
      Return a list of months between two dates in YYYYMM format.
      Use to convert from some start end date to a list of months

      Args:
               start (str): Start date in YYYYMM format
               end (str): End date in YYYYMM format

      Returns:
               month_list ([]str): A list of month between the two months input (Inclusive)

      Example:
               month_list = get_months_between_dates('201802', '201902')

      Example output:
               ['201802', '201803', '201804', '201805', '201806', '201807', '201808', '201809', '201810', '201811', '201812', '201901', '201902']
      """

      start_dt = datetime.strptime(start, "%Y%m")
      end_dt   = datetime.strptime(end, "%Y%m")
      month_list = [dt.strftime("%Y%m") for dt in rrule(MONTHLY, dtstart=start_dt, until=end_dt)]
      return month_list

#+END_SRC

*** R Docstrings

For R, I find that the standard documentation format is kinda hard to read (personal preference), so I follow the python docstring format as well.

One of the draw back would be you cant read the documentation from the ~help~ function natively supported by R. But it gives you a more consistent feel between Python and R projects.


#+BEGIN_SRC r
  library(data.table)
  round_dataframe <- function(df, digits = 2) {

    ## Round the numeric columns of the provided dataframe
    ##
    ## Args:
    ##  df (Dataframe): Dataframe to be rounded
    ##  digits (num): No of digits to be rounded for numeric columns
    ##
    ## Returns:
    ##  df (Dataframe): Dataframe with rounded numbers
    ##
    ## Example:
    ##  dt = as.data.table(iris)
    ##  dt = round_dataframe(df = dt, digits = 0)

    # Find numeric cols, round the columns with no of digits provided
    numeric.cols = colnames(Filter(is.numeric, df))
    df[, (numeric.cols) := round(.SD, digits), .SDcols = numeric.cols]

    return(df)
  }

#+END_SRC

Reference: [[https://style.tidyverse.org/
][R tidyverse style guide]]

*** Go Docstrings

For Go, you can easily tell from the function signature about the input, output and description of the function.
It also comes with some built-in support for documentation. You can easily generate the doc with ~go doc --all~ or ~godoc -http=localhost:7000~.


#+BEGIN_SRC go

  // Add simply adds the two integers together
  func Add(x int, y int) int{
          return x + y
  }

#+END_SRC

*** Demo

Writing docstrings might sound a lot of trouble at first. But it may not be as much as you think. Many modern IDE supports some code snippets for you to define a template for code generation.

Here is a quick demo on how I usually generate the docstrings with **yasnippet**. I group all my snippets starting with the letter ~s~ for snippets.
- ~sifm~ prints the ~if __name__ == '__main__'~
- ~sfn~ stands for snippet function, which generate a docstring snippet whenever I define a function.

[[https://storage.googleapis.com/billylkc-blog-image/images/posts/10-docstrings/docstrings-o.gif]]

*** Final Thoughts

Hopefully, you start to see the benefits of writing a good function signature after this post. \\
Happy Coding!

_

Reference Photo by [Gustavo Fring](https://www.pexels.com/@gustavo-fring?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels) on [Pexels](https://www.pexels.com/photo/clever-little-student-writing-in-notebook-while-studying-at-home-3874375/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels)



** TODO Empty template
:PROPERTIES:
:EXPORT_FILE_NAME: file-name
:EXPORT_DATE: 2021-02-22
:EXPORT_HUGO_MENU: :menu "main"
:EXPORT_HUGO_WEIGHT: 1000
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :nolastmod true :cover https://storage.googleapis.com/billylkc-blog-image/images/posts/4-functional-options/thumbnails.jpg
:DESCRIPTION: description
:EXPORT_TITLE: description
:SUMMARY: description
:END:

Some short description

<!--more-->
